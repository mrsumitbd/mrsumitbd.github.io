<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://mrsumitbd.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mrsumitbd.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-20T02:20:45+00:00</updated><id>https://mrsumitbd.github.io/feed.xml</id><title type="html">Musfiqur Rahman</title><subtitle>PhD Candidate in Software Engineering</subtitle><entry><title type="html">Data Version Control (DVC): The Action and The Reaction</title><link href="https://mrsumitbd.github.io/blog/2025/dvc-action-reaction/" rel="alternate" type="text/html" title="Data Version Control (DVC): The Action and The Reaction"/><published>2025-12-18T19:46:00+00:00</published><updated>2025-12-18T19:46:00+00:00</updated><id>https://mrsumitbd.github.io/blog/2025/dvc-action-reaction</id><content type="html" xml:base="https://mrsumitbd.github.io/blog/2025/dvc-action-reaction/"><![CDATA[<p>Machine Learning (ML) engineering has a â€œdata problem.â€ Unlike traditional software, where versioning code is solved by Git, ML systems depend heavily on large datasets that Git struggles to handle. Tools like <strong>Data Version Control (DVC)</strong> have emerged to bridge this gap, promising to manage data, models, and pipelines as seamlessly as code.</p> <p>But how are developers actually using these tools in the wild? And does adopting them actually improve the development process?</p> <p>In our paper, <strong>â€œDVC in Open Source ML-development: The Action and the Reaction,â€</strong> presented at <strong>CAIN 2024</strong>, we conducted an empirical study of 56 real-world open-source projects to find the answers.</p> <h3 id="the-action-how-is-dvc-used">The Action: How is DVC Used?</h3> <p>We analyzed the usage patterns of DVC (â€œThe Actionâ€) and found a clear preference. While DVC offers features for both <strong>data versioning</strong> and <strong>pipeline management</strong>, the community overwhelmingly uses it for the former.</p> <ul> <li><strong>85.7%</strong> of projects use DVC exclusively for <strong>versioning and tracking</strong> (mostly data files).</li> <li><strong>Zero</strong> projects used it solely for pipelines.</li> <li>Only <strong>14.3%</strong> used both features.</li> </ul> <p>This suggests that while DVC is the â€œgo-toâ€ for handling large files, practitioners often prefer other tools (like MLflow) for managing their experiment pipelines.</p> <h3 id="the-reaction-how-does-it-impact-workflow">The Reaction: How Does It Impact Workflow?</h3> <p>We also performed an Interrupted Time-Series (ITS) analysis to measure the impact of adopting DVC on the projectâ€™s health (â€œThe Reactionâ€). The results revealed two fascinating trends:</p> <ol> <li><strong>Fewer Pull Requests (PRs):</strong> After adopting DVC, the number of PRs consistently dropped. This is actually a positive sign of <strong>decoupling</strong>â€”data scientists can now update datasets independently without needing to open a code-change PR for every data tweak.</li> <li><strong>The â€œSpike then Dropâ€ in Bugs:</strong> We observed a sudden spike in bug-fix commits immediately after adoption (the â€œteething painsâ€ of integration), followed by a sustained decrease. This indicates that while the initial setup requires effort, DVC leads to a more stable repository in the long run.</li> </ol> <p>DVC is clearly more than just a storage tool; it fundamentally alters the rhythm of ML collaboration.</p> <hr/> <details> <summary><strong>Cite the paper</strong></summary> <br/> <pre><code>@inproceedings{barreto2024dvc,<br />
  title={DVC in Open Source ML-development: The Action and the Reaction},<br />
  author={Barreto Simedo Pacheco, Lorena and Rahman, Musfiqur and Rabbi, Fazle and Fathollahzadeh, Pouya and Abdellatif, Ahmad and Shihab, Emad and Chen, Tse-Hsun and Yang, Jinqiu and Zou, Ying},<br />
  booktitle={Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering-Software Engineering for AI},<br />
  pages={75--80},<br />
  year={2024}<br />
}</code></pre> </details>]]></content><author><name></name></author><category term="publications"/><category term="research"/><category term="mlops"/><category term="software-engineering"/><summary type="html"><![CDATA[An empirical study on how open-source developers actually use DVC and how it impacts their workflow.]]></summary></entry><entry><title type="html">The Reality Check: Why AI Struggles with Real-World Software Engineering</title><link href="https://mrsumitbd.github.io/blog/2025/beyond-synthetic-benchmarks/" rel="alternate" type="text/html" title="The Reality Check: Why AI Struggles with Real-World Software Engineering"/><published>2025-12-18T19:30:00+00:00</published><updated>2025-12-18T19:30:00+00:00</updated><id>https://mrsumitbd.github.io/blog/2025/beyond-synthetic-benchmarks</id><content type="html" xml:base="https://mrsumitbd.github.io/blog/2025/beyond-synthetic-benchmarks/"><![CDATA[<p>If you have ever used tools like GitHub Copilot or ChatGPT to write code, you know the feeling: it feels like magic when generating a standalone Python function, but often falls apart when asked to build a complex class that integrates with an existing project.</p> <p>A new paper, <strong>â€œBeyond Synthetic Benchmarks,â€</strong> confirms this intuition with hard data. It reveals a startling gap between how Large Language Models (LLMs) perform on artificial tests versus real-world software engineering tasks.</p> <p><strong>Paper:</strong> <a href="https://arxiv.org/pdf/2510.26130">Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation</a><br/> <strong>Authors:</strong> Musfiqur Rahman, SayedHassan Khatoonabadi, Emad Shihab</p> <hr/> <h3 id="the-leetcode-illusion">The â€œLeetCodeâ€ Illusion</h3> <p>Current AI benchmarks, such as HumanEval or MBPP, typically ask models to solve self-contained algorithmic puzzles (e.g., â€œWrite a function to reverse a stringâ€). These â€œsyntheticâ€ benchmarks have reported success rates as high as <strong>89%</strong> for top-tier models.</p> <p>However, real software development is rarely about isolated functions. It involves <strong>class-level engineering</strong>: managing state, handling inheritance, and integrating with external libraries and project-specific dependencies. To bridge this gap, we introduced a novel benchmark derived from real-world open-source repositories, partitioning data into â€œseenâ€ (older code) and â€œunseenâ€ (recent code) to rigorously test generalization.</p> <h3 id="key-findings-the-89-vs-25-gap">Key Findings: The 89% vs. 25% Gap</h3> <p>The results serve as a reality check for the industry. When tested on this new <strong>RealClassEval</strong> benchmark, LLMs that scored <strong>84â€“89%</strong> on synthetic tasks plummeted to a mere <strong>25â€“34%</strong> accuracy on real-world class-level generation.</p> <p>This huge disparity highlights a critical limitation: models are excellent at memorizing syntax and small patterns but struggle with the â€œarchitecturalâ€ complexity of maintaining class invariants and dependencies. Interestingly, the study found negligible performance differences between â€œseenâ€ and â€œunseenâ€ codebases, suggesting that the models arenâ€™t simply failing because they havenâ€™t seen the code beforeâ€”they are failing because the task itself is fundamentally harder.</p> <h3 id="does-rag-or-documentation-help">Does RAG or Documentation Help?</h3> <p>We also explored if giving the model more helpâ€”via comprehensive documentation or Retrieval-Augmented Generation (RAG)â€”could close the gap.</p> <ul> <li><strong>Documentation:</strong> Surprisingly, adding comprehensive docstrings yielded only modest gains of <strong>1â€“3%</strong> in functional accuracy.</li> <li><strong>RAG:</strong> Retrieval-augmented generation proved more effective, specifically when documentation was incomplete. By retrieving relevant snippets from the codebase, RAG improved correctness by <strong>4â€“7%</strong>, effectively supplying the â€œconcrete implementation patternsâ€ that abstract prompts missed.</li> </ul> <h3 id="common-failures">Common Failures</h3> <p>An error analysis revealed that <strong>84%</strong> of failures in real-world scenarios were due to <code class="language-plaintext highlighter-rouge">AttributeError</code>, <code class="language-plaintext highlighter-rouge">TypeError</code>, and <code class="language-plaintext highlighter-rouge">AssertionError</code>. Unlike synthetic benchmarks where logic errors dominate, real-world failures were often structuralâ€”models hallucinated attributes that didnâ€™t exist or misused types, proving they often lack a deep â€œmental modelâ€ of the class structure they are building.</p> <h3 id="conclusion">Conclusion</h3> <p>This paper is a wake-up call for researchers and developers. High scores on synthetic benchmarks do not guarantee readiness for production environments. As we move toward autonomous AI software engineers, we must move â€œbeyond synthetic benchmarksâ€ and start evaluating models on the messy, complex, and interdependent reality of actual software development.</p> <hr/> <hr/> <details> <summary><strong>Cite the paper</strong></summary> <br/> <pre><code>@inproceedings{rahman2019natural,<br />
  title={Natural software revisited},<br />
  author={Rahman, Musfiqur and Palani, Dharani and Rigby, Peter C},<br />
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},<br />
  pages={37--48},<br />
  year={2019},<br />
  organization={IEEE}
}</code></pre> </details>]]></content><author><name></name></author><category term="publications"/><category term="research"/><category term="llm"/><category term="software-engineering"/><summary type="html"><![CDATA[A deep dive into my latest paper on the gap between synthetic benchmarks and real-world code generation.]]></summary></entry><entry><title type="html">Why Your AI Model Costs More on a Different Machine</title><link href="https://mrsumitbd.github.io/blog/2025/impact-of-env-configurations/" rel="alternate" type="text/html" title="Why Your AI Model Costs More on a Different Machine"/><published>2025-12-17T12:00:00+00:00</published><updated>2025-12-17T12:00:00+00:00</updated><id>https://mrsumitbd.github.io/blog/2025/impact-of-env-configurations</id><content type="html" xml:base="https://mrsumitbd.github.io/blog/2025/impact-of-env-configurations/"><![CDATA[<p><em>Preprint available on <a href="https://arxiv.org/abs/2408.02825">arXiv</a>.</em></p> <h2 id="the-30-second-summary">The 30-Second Summary</h2> <p><strong>TL;DR:</strong> Developers often worry that moving an AI model from Linux to macOS (or changing Python versions) will break its accuracy. We found that while accuracy <em>can</em> drop (in ~23% of cases), the real danger is <strong>Instability in Time and Cost</strong>. Changing your environment configuration can silently double your processing time or cloud costs in nearly <strong>100% of cases</strong>, without you realizing it until the bill arrives.</p> <hr/> <h2 id="the-problem">The Problem</h2> <p>â€œIt works on my machineâ€ is the classic developer excuse. In traditional software, if the environment changes (e.g., different OS), the code usually crashes immediately.</p> <p>But AI systems are â€œnon-deterministic.â€ If you change the underlying hardware (CPU architecture) or the software stack (Python version, OS), the system might not crash. Instead, it might:</p> <ol> <li>Give slightly different predictions (Model Instability).</li> <li>Take twice as long to run (Time Instability).</li> <li>Cost significantly more to compute (Expense Instability).</li> </ol> <p>We wanted to know: <strong>Which of these actually happens, and how bad is it?</strong></p> <h2 id="our-approach">Our Approach</h2> <p>We conducted an extensive empirical study using <strong>Travis CI</strong>.</p> <ul> <li><strong>Subjects:</strong> 30 Open Source AI-enabled systems.</li> <li><strong>Variables:</strong> We permuted 8 different environment configurations, changing the <strong>Operating System</strong> (Linux vs. macOS), <strong>Python Version</strong>, and <strong>CPU Architecture</strong>.</li> <li><strong>Metrics:</strong> We measured shifts in Model Performance (accuracy), Processing Time, and Total Expense.</li> </ul> <h2 id="key-results">Key Results</h2> <p>Our findings were surprising. Most people expect the model to â€œbreakâ€ (lose accuracy), but that wasnâ€™t the biggest issue.</p> <ol> <li><strong>Accuracy is (Mostly) Safe:</strong> When switching from Linux to macOS, only <strong>23%</strong> of projects saw a significant change in model performance.</li> <li><strong>Time is Volatile:</strong> <strong>96.67%</strong> of projects experienced significant instability in processing time. A model that runs fast on Linux might crawl on macOS.</li> <li><strong>Cost is Guaranteed to Change:</strong> <strong>100%</strong> of the studied projects saw expense instability.</li> </ol> <h2 id="why-this-matters">Why This Matters</h2> <p>For <strong>DevOps Engineers and ML Practitioners</strong>, this is a wake-up call.</p> <ul> <li><strong>Donâ€™t just check accuracy:</strong> Passing your unit tests isnâ€™t enough. You need to benchmark <em>time</em> and <em>cost</em> across your deployment environments.</li> <li><strong>Standardize early:</strong> Since configurations drastically impact cost, finding the â€œOptimal Configurationâ€ isnâ€™t just about performanceâ€”itâ€™s about financial efficiency.</li> <li><strong>Containerize:</strong> To avoid these â€œsilentâ€ cost spikes, use Docker to ensure your production environment matches your testing environment exactly.</li> </ul> <h2 id="resources">Resources</h2> <ul> <li><a href="https://arxiv.org/abs/2408.02825">ğŸ“„ Read the Paper (arXiv)</a></li> <li><a href="https://github.com/mrsumitbd/Impact-of-Environment-Configurations">ğŸ’» View the Code</a></li> </ul> <hr/> <details> <summary><strong>Cite this post</strong></summary> <br/> <pre><code>@article{rahman2024impact,<br />
  title={The Impact of Environment Configurations on the Stability of AI-Enabled Systems},<br />
  author={Rahman, Musfiqur and Khatoonabadi, SayedHassan and Abdellatif, Ahmad and Samaana, Haya and Shihab, Emad},<br />
  journal={arXiv preprint arXiv:2408.02825},<br />
  year={2024}
}</code></pre> </details>]]></content><author><name></name></author><category term="publications"/><category term="research"/><category term="ai"/><category term="stability"/><category term="devops"/><summary type="html"><![CDATA[We tested 30 AI systems across different environments. The result? Your accuracy might stay safe, but your cloud bill won't.]]></summary></entry><entry><title type="html">Is Code Really More â€˜Naturalâ€™ Than English?</title><link href="https://mrsumitbd.github.io/blog/2025/natural-software-revisited/" rel="alternate" type="text/html" title="Is Code Really More â€˜Naturalâ€™ Than English?"/><published>2025-12-17T12:00:00+00:00</published><updated>2025-12-17T12:00:00+00:00</updated><id>https://mrsumitbd.github.io/blog/2025/natural-software-revisited</id><content type="html" xml:base="https://mrsumitbd.github.io/blog/2025/natural-software-revisited/"><![CDATA[<p><em>Published at <a href="http://users.encs.concordia.ca/~pcr/paper/Rahman2019ICSE-draft.pdf">ICSE 2019</a>.</em></p> <h2 id="the-30-second-summary">The 30-Second Summary</h2> <p><strong>TL;DR:</strong> Previous research claimed software is huge amounts more repetitive (â€œnaturalâ€) than English. We found that this is mostly an illusion caused by rigid syntax like semicolons and braces. When you strip those away, code is only slightly more repetitive than technical English. However, <strong>API usages</strong> remain highly predictable, suggesting that future AI tools should focus on graphs of API calls rather than simple text prediction.</p> <hr/> <h2 id="the-problem">The Problem</h2> <p>In 2012, a groundbreaking study coined the term â€œNatural Software,â€ showing that code is highly repetitiveâ€”much more so than English text. This led to a boom in statistical tools that treat code like text (e.g., n-gram models for autocompletion).</p> <p>But here is the catch: <strong>Code has strict grammar rules.</strong> If a tool predicts a <code class="language-plaintext highlighter-rouge">;</code> at the end of a line, is it really â€œunderstandingâ€ your code, or just following a rule? We suspected that existing models were â€œcheatingâ€ by predicting easy syntax tokens rather than meaningful logic.</p> <h2 id="our-approach">Our Approach</h2> <p>We revisited the original study but added a twist. We analyzed 134 open-source projects across 7 languages (Java, C, Python, Ruby, etc.) and technical discussions from StackOverflow.</p> <ul> <li><strong>Step 1:</strong> We replicated the original n-gram experiments to establish a baseline.</li> <li><strong>Step 2:</strong> We systematically removed â€œSyntaxTokensâ€ (separators, operators, keywords) to measure the â€œtrueâ€ entropy of the logic.</li> <li><strong>Step 3:</strong> We compared sequential models (n-grams) against <strong>Graph-based Object Usage Models (GROUMs)</strong> to see if graphs captured patterns better.</li> </ul> <h2 id="key-results">Key Results</h2> <p>Our data tells a different story about code repetitiveness:</p> <ol> <li><strong>Itâ€™s the Syntax:</strong> When we removed syntax tokens (like <code class="language-plaintext highlighter-rouge">{</code>, <code class="language-plaintext highlighter-rouge">}</code>, <code class="language-plaintext highlighter-rouge">;</code>), the â€œunnaturalâ€ repetitiveness of code dropped distinctively. [cite_start]Java code without syntax is only ~10-20% more repetitive than technical English on StackOverflow[cite: 238, 250].</li> <li><strong>APIs are Key:</strong> While general code isnâ€™t super repetitive, <strong>API usages</strong> are. [cite_start]Using a file input stream looks almost identical across thousands of different projects[cite: 252, 284].</li> <li><strong>Graphs Win:</strong> Compilers donâ€™t read code left-to-right, and neither should AI. [cite_start]We found that <strong>graph representations</strong> of code were significantly more repetitive (predictable) than text sequences, capturing relationships that n-grams miss[cite: 378, 403].</li> </ol> <h2 id="why-this-matters">Why This Matters</h2> <p>For software engineers and tool builders, this changes how we build intelligent assistants:</p> <ul> <li><strong>Stop predicting semicolons:</strong> Tools should focus on â€œhigh-valueâ€ predictions, like API calls and method chains, rather than syntax that a compiler can fix.</li> <li><strong>Think in Graphs:</strong> Since graphs capture more signal than text, the next generation of â€œCopilotsâ€ should leverage structural information (Data/Control flow) rather than just treating code as a sequence of words.</li> </ul> <h2 id="resources">Resources</h2> <ul> <li><a href="http://users.encs.concordia.ca/~pcr/paper/Rahman2019ICSE-draft.pdf">ğŸ“„ Read the Paper (PDF)</a></li> <li><a href="https://www.dropbox.com/s/o5016gelg8tx5yx/README.md?dl=0">ğŸ’» Replication Package</a></li> </ul> <hr/> <hr/> <details> <summary><strong>Cite the paper</strong></summary> <br/> <pre><code>@inproceedings{rahman2019natural,<br />
  title={Natural software revisited},<br />
  author={Rahman, Musfiqur and Palani, Dharani and Rigby, Peter C},<br />
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},<br />
  pages={37--48},<br />
  year={2019},<br />
  organization={IEEE}
}</code></pre> </details>]]></content><author><name></name></author><category term="publications"/><category term="research"/><category term="software-engineering"/><category term="nlp"/><summary type="html"><![CDATA[We revisited the famous 'Natural Software' hypothesis and found that syntax rulesâ€”not logicâ€”drive most of the repetition in code.]]></summary></entry></feed>